---
title: "Predict Attrition in Credit Customers"
author: "Jacques Anthony MS"

# Read in data
```{r echo=FALSE, cache=TRUE}
setwd("~/Credit Card Customers")
library(readxl)
library(tinytex)
library(knitr)
mydata = read.csv("BankChurners.csv",stringsAsFactors = F)
feat <- read_xlsx("Feature Description.xlsx")
knitr::kable(feat, caption = "Customer Attributes with Description")
```

## Response variable (Attrition Flag)
```{r, echo=FALSE, fig.align = 'center', fig.height = 2, fig.width=3,fig.cap= "Proportions of Existing and Attrited Customers with 8500 existing customers and 1627 for attrited customers", out.width="33%", cache=TRUE }
mydata = mydata[,-c(22,23)]
library(ggplot2)
ggplot(mydata) + aes(Attrition_Flag) + geom_bar() + xlab("Attrition Flag")
```
# Frequencies for Categorical variables
```{r ,echo=FALSE, fig.align='center', fig.cap= "Frequencies of different classes contained within each categorical variable", fig.height=4, fig.width = 9, cache=TRUE}
library(DataExplorer)
plot_bar(mydata)
```
# Boxplots
```{r, echo=FALSE, figures-side, fig.show="hold", out.width="50%", fig.cap= "Credit Limit, Transaction Counts, Average Utilization Ratio, and Months of Inactivity in the last 12 months for Attrited and Existing Customers", fig.height=1.5, cache=TRUE, fig.width=2.9, fig.align='center'}

trio = mydata[,c("Attrition_Flag","Credit_Limit","Total_Trans_Ct","Avg_Utilization_Ratio","Months_Inactive_12_mon")]
ggplot(data=trio, mapping=aes(x=Attrition_Flag, y=Credit_Limit), size =2)+geom_boxplot() 

ggplot(data=trio, mapping=aes(x=Attrition_Flag, y=Total_Trans_Ct), size =2)+geom_boxplot()  + ylab("Transaction Counts")

ggplot(data=trio, mapping=aes(x=Attrition_Flag,y=Avg_Utilization_Ratio), size =2)+geom_boxplot() + ylab("Average Utilization")

ggplot(data=trio, mapping=aes(x=Attrition_Flag,y=Months_Inactive_12_mon), size =2)+geom_boxplot() + ylab("Months Inactive")
```


# Remove variable clientnum

```{r include=FALSE, cache=TRUE}
library(dplyr)
mydata = mydata[,-1]
mydata = distinct(mydata)
```

# Data normalization and one hot encoding
```{r include=FALSE, cache=TRUE}
library(caret)
predata <- preProcess(mydata[,-c(1,3,5,6,7,8)],method = c("range"))
ndata <- predict(predata,mydata[,-c(1,3,5,6,7,8)])
mydatanorm <- cbind(ndata,mydata[,c(1,3,5,6,7,8)])
mydatanorm[sapply(mydatanorm, is.character)] <- lapply(mydatanorm[
  sapply(mydatanorm, is.character)], as.factor)
```

# Correlation Plot
```{r ,echo = FALSE, fig.align='center', fig.cap='Correlation Plot for Numerical Variables showing strong correlation (score=1) between Credit Limit and Average Open to Buy', fig.height=10, fig.width=8, cache=TRUE}
library(DataExplorer)
plot_correlation(mydatanorm, type = "c")
```
# Investigate correlation between credit limit and average open to buy
```{r echo=FALSE,cache=TRUE, fig.cap="Correlation Plot between credit limit and average open to buy. It indicates a strong correlation (score=1) because of the straight line", fig.height = 2, fig.width=2, fig.align='center',}
ggplot(data = mydatanorm, aes(x = Credit_Limit, y = Avg_Open_To_Buy)) +
    geom_point() + labs(x="Credit Limit", y="Average Open to Buy")
```

# Feature Selection with LASSO

```{r include = FALSE, cache=TRUE}
library(glmnet)
y <- mydatanorm$Attrition_Flag
x <- data.matrix(mydatanorm[, -15])
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1, family = 'binomial', nlambda = 100)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
##plot(cv_model)
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 1, family = 'binomial',lambda = best_lambda)
coef(best_model)
```

# LASSO Coefficients
```{r, echo=FALSE, cache=TRUE}
library(caret)
library(knitr)
lasso = varImp(best_model, lambda = best_lambda)
colnames(lasso) <- "Lasso Coefficients"
lasso$`Lasso Coefficients` = round(lasso$`Lasso Coefficients`,2)
kable(lasso, caption = "Lasso Coefficients for each variable. It shows that Average Open to Buy and Income have a coefficient of zero.")
```
# Removed variables after LASSO
```{r include = FALSE, cache=TRUE}
mydatafinal <- mydatanorm[,-c(1,9,11,14,19)]
```

# Data partioning: training and test sets
```{r include = FALSE, cache=TRUE}
library(caTools)
set.seed(99)
split = sample.split(mydatafinal$Attrition_Flag, SplitRatio = 0.8)
train = subset(mydatafinal, split == TRUE)
test = subset(mydatafinal, split == FALSE)
#train$Attrition_Flag = ifelse(train$Attrition_Flag == "Existing Customer",1,0)
#train$Attrition_Flag = as.factor(train$Attrition_Flag)
```
# SMOTE for oversampling of the minority class
```{r include = FALSE, cache=TRUE}
library(smotefamily)
set.seed(123)
smote_result = SMOTE(X = train[, -c(11:15)], target = train$Attrition_Flag, K = 5, dup_size = 0)
train_oversampled = smote_result$data
colnames(train_oversampled)[11] = "Attrition_Flag"
table(train_oversampled$Attrition_Flag)
prop.table(table(train_oversampled$Attrition_Flag))
train_oversampled$Attrition_Flag = as.factor(train_oversampled$Attrition_Flag)

```


# Logistic function 
```{r, echo=FALSE, cache=TRUE, warning=FALSE , fig.align='center', fig.height=5, fig.width=4, message=FALSE, fig.cap="The plot of attrition flag vs total transaction counts demonstrates a logistic regression can be fitted to the data. The blue curve shows the smooth line fitted to the data"}
library(dplyr)
library(ggplot2)
train %>% mutate(prob = ifelse(Attrition_Flag == "Existing Customer",1,0)) %>% ggplot(aes(Total_Trans_Ct, prob)) + geom_point(alpha = 0.2) + geom_smooth(method = "glm", method.args = list(family = "binomial")) + labs( x = "Total Transaction Counts", y = "Attrition Flag")
```
# Logistic Regression
```{r include = FALSE,cache=TRUE}
library(glmnet)
set.seed(25)
logmodel <- glm(Attrition_Flag~., data = train_oversampled, family = binomial)
predlog <- predict(logmodel, test, type="response")
contrasts(test$Attrition_Flag)
predlogres <- ifelse(predlog > 0.5, 1, 0)
table(predlogres, test$Attrition_Flag)
```

# Logistic Regression Coefficients
```{r, echo=FALSE, cache=TRUE}
library(knitr)
logsum = summary(logmodel)$coef
lognew = as.data.frame(logsum)
lognew[,-4] = round(lognew[,-4],2)
lognew$`Pr(>|z|)` = round(lognew$`Pr(>|z|)`,4)
kable(lognew, caption = "Variables coefficients from Logistic Regression. It shows that the variable total amount change Q4 over Q1 is insignificant.")
```
# Confusion Matrix for Logistic Regression 
```{r, echo=FALSE, cache=TRUE}
kable(table(predlogres, test$Attrition_Flag), caption = "Confusion Matrix for Logistic Regression")
```

# Random Forests
```{r include = FALSE, cache=TRUE}
memory.limit(35000)
library(randomForest)
set.seed(25)
rfmodel <- randomForest(Attrition_Flag ~., data = train_oversampled, importance = TRUE, proximity = TRUE, ntree = 10)
predrf <- predict(rfmodel,test)
```
















































































































































































